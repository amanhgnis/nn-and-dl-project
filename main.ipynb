{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recoloring with Conditional GANs\n",
    "<b>Reference paper</b>:\n",
    "Phillip Isola et. Al. “Image to Image Translation with\n",
    "Conditional Adversarial Networks”, CVPR 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI-E6uHzDqA8"
   },
   "outputs": [],
   "source": [
    "!pip3 install opencv-python-headless\n",
    "!pip3 install torchmetrics[image]\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3LHVgNCHUac",
    "outputId": "2935e2cc-c0b7-4ac9-da54-b0912ba42bb4"
   },
   "outputs": [],
   "source": [
    "# Download COCO dataset\n",
    "!mkdir data\n",
    "\n",
    "print(\"=\"*51 + \"DOWNLOADING TRAINING SET\" + \"=\"*51)\n",
    "!wget -c http://images.cocodataset.org/zips/train2017.zip -P ./data\n",
    "!unzip -qq ./data/train2017.zip -d ./data\n",
    "!rm ./data/train2017.zip\n",
    "\n",
    "print(\"=\"*50 + \"DOWNLOADING VALIDATION SET\" + \"=\"*50)\n",
    "!wget -c http://images.cocodataset.org/zips/val2017.zip -P ./data\n",
    "!unzip -qq ./data/val2017.zip -d ./data\n",
    "!rm ./data/val2017.zip\n",
    "\n",
    "print(\"=\"*53 + \"DOWNLOADING TEST SET\" + \"=\"*53)\n",
    "!wget -c http://images.cocodataset.org/zips/test2017.zip -P ./data\n",
    "!unzip -qq ./data/test2017.zip -d ./data\n",
    "!rm ./data/test2017.zip\n",
    "\n",
    "print(\"=\"*59 + \"FINISHED\" + \"=\"*59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUlPnNhGoc7x"
   },
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root:str, color_space:str = \"RGB\", size_limit=None, transform=None):\n",
    "        self.paths = glob.glob(root+\"/*.jpg\")\n",
    "        if size_limit != None:\n",
    "            self.paths = self.paths[:size_limit]\n",
    "        self.transform = transform\n",
    "        self.color_space = color_space\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.paths[index]\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.color_space == \"RGB\":\n",
    "            image = np.array(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            target_img = image\n",
    "        if self.color_space == \"Lab\":\n",
    "            image = np.array(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "            img = image[:,:,[0]]\n",
    "            target_img = image[:,:,[1,2]]\n",
    "\n",
    "        img = transforms.ToTensor()(img)\n",
    "        target_img = transforms.ToTensor()(target_img)\n",
    "        img = 2.0 * img - 1.0\n",
    "        target_img = 2.0 * target_img - 1.0\n",
    "        return (img, target_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PogKbt3DEovO"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "])\n",
    "\n",
    "\n",
    "RGB_test_set = COCODataset(root=\"./data/test2017\", color_space=\"RGB\", transform=test_transforms)\n",
    "Lab_test_set = COCODataset(root=\"./data/test2017\", color_space=\"Lab\", transform=test_transforms)\n",
    "\n",
    "index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "9G-DYqhpBnJ8",
    "outputId": "63052cba-62a3-4a50-8701-1c7468e25897"
   },
   "outputs": [],
   "source": [
    "grayscale_image, image = RGB_test_set[index]\n",
    "\n",
    "grayscale_image = (grayscale_image + 1.0) / 2.0\n",
    "image = (image + 1.0) / 2.0\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15,15))\n",
    "ax[0].imshow(transforms.ToPILImage()(grayscale_image), cmap='gray')\n",
    "ax[1].imshow(transforms.ToPILImage()(image))\n",
    "ax[2].imshow(transforms.ToPILImage()(image[[0],:,:]), cmap='Reds')\n",
    "ax[3].imshow(transforms.ToPILImage()(image[[1],:,:]), cmap='Greens')\n",
    "ax[4].imshow(transforms.ToPILImage()(image[[2],:,:]), cmap='Blues')\n",
    "ax[0].set_title(\"Grayscale\")\n",
    "ax[1].set_title(\"Colored\")\n",
    "ax[2].set_title(\"Red channel\")\n",
    "ax[3].set_title(\"Green channel\")\n",
    "ax[4].set_title(\"Blue channel\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].axis(\"off\")\n",
    "ax[3].axis(\"off\")\n",
    "ax[4].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(f\"Input (Grayscale image) shape: {grayscale_image.shape}\")\n",
    "print(f\"Label (Colored image) shape: {image.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "8N-jK0T4xkT4",
    "outputId": "223b323f-d0e6-4916-cab1-5a6c7c98f82f"
   },
   "outputs": [],
   "source": [
    "L_image, ab_image = Lab_test_set[index]\n",
    "\n",
    "L_image = (L_image + 1.0) / 2.0\n",
    "ab_image = (ab_image + 1.0) / 2.0\n",
    "\n",
    "colored_image = transforms.ToPILImage()(torch.cat([L_image, ab_image]))\n",
    "colored_image = np.array(colored_image)\n",
    "colored_image = cv2.cvtColor(colored_image, cv2.COLOR_Lab2RGB)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,15))\n",
    "ax[0].imshow(transforms.ToPILImage()(colored_image))\n",
    "ax[1].imshow(transforms.ToPILImage()(L_image), cmap='gray')\n",
    "ax[2].imshow(transforms.ToPILImage()(image[[0],:,:]), cmap='Reds')\n",
    "ax[3].imshow(transforms.ToPILImage()(image[[1],:,:]), cmap='Blues')\n",
    "ax[0].set_title(\"Colored\")\n",
    "ax[1].set_title(\"L channel\")\n",
    "ax[2].set_title(\"a channel\")\n",
    "ax[3].set_title(\"b channel\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].axis(\"off\")\n",
    "ax[3].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "print(f\"Input (L channel) shape: {L_image.shape}\")\n",
    "print(f\"Label (ab channels) shape: {ab_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ao88xiKR2TK0",
    "outputId": "ed23321d-b764-479b-8010-eca6830bbe87"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "IMG_SIZE = 128\n",
    "COLOR_SPACE = \"Lab\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "])\n",
    "\n",
    "\n",
    "dataset = COCODataset(root=\"./data/test2017\", color_space=COLOR_SPACE, size_limit=11000, transform=train_transforms)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [10000, 1000])\n",
    "test_set = COCODataset(root=\"./data/val2017\", color_space=COLOR_SPACE, size_limit=1000, transform=test_transforms)\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True, shuffle=True)\n",
    "val_dl = DataLoader(val_set, batch_size=BATCH_SIZE,num_workers=2, pin_memory=True, shuffle=False)\n",
    "test_dl = DataLoader(test_set, batch_size=8, num_workers=2, pin_memory=True, shuffle=False)\n",
    "\n",
    "print(f\"Training set has {len(train_set)} images\")\n",
    "print(f\"Validation set has {len(val_set)} images\")\n",
    "print(f\"Test set has {len(test_set)} images\")\n",
    "print()\n",
    "for sample in test_dl:\n",
    "    x, y = sample\n",
    "    print(f\"Input batch has shape: {x.shape}\")\n",
    "    print(f\"Output batch has shape{y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net autoencoder for generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, use_bias=False, use_dropout=True):\n",
    "        \"\"\"\n",
    "        :param input_nc: number of input channels\n",
    "        :param output_nc: number of output channels\n",
    "        :param ngf: number of generator filters in the first convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.downrelu = nn.LeakyReLU(0.2, True)\n",
    "        self.uprelu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.drop_rate = 0.5 if use_dropout else 0.0\n",
    "        \n",
    "        self.downconv1 = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downconv2 = nn.Conv2d(ngf, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn2   = nn.BatchNorm2d(ngf*2)\n",
    "        self.downconv3 = nn.Conv2d(ngf*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn3   = nn.BatchNorm2d(ngf*4)\n",
    "        self.downconv4 = nn.Conv2d(ngf*4, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn4   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv5 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn5   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv6 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn6   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv7 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn7   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv8 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        \n",
    "        self.upconv1   = nn.ConvTranspose2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn1     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop1   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv2   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn2     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop2   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv3   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn3     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop3   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv4   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn4     = nn.BatchNorm2d(ngf*8)\n",
    "        self.upconv5   = nn.ConvTranspose2d(ngf*8*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn5     = nn.BatchNorm2d(ngf*4)\n",
    "        self.upconv6   = nn.ConvTranspose2d(ngf*4*2, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn6     = nn.BatchNorm2d(ngf*2)\n",
    "        self.upconv7   = nn.ConvTranspose2d(ngf*2*2, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn7     = nn.BatchNorm2d(ngf)\n",
    "        self.upconv8   = nn.ConvTranspose2d(ngf*2, output_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        e1  = self.downconv1(x)                                        # input x is (input_nc) x 256 x 256\n",
    "        e2  = self.downbn2(self.downconv2(self.downrelu(e1)))          # input e1 is (ngf) x 128 x 128\n",
    "        e3  = self.downbn3(self.downconv3(self.downrelu(e2)))          # input e2 is (ngf * 2) x 64 x 64\n",
    "        e4  = self.downbn4(self.downconv4(self.downrelu(e3)))          # input e3 is (ngf * 4) x 32 x 32\n",
    "        e5  = self.downbn5(self.downconv5(self.downrelu(e4)))          # input e4 is (ngf * 8) x 16 x 16\n",
    "        e6  = self.downbn6(self.downconv6(self.downrelu(e5)))          # input e5 is (ngf * 8) x 8 x 8\n",
    "        e7  = self.downbn7(self.downconv7(self.downrelu(e6)))          # input e6 is (ngf * 8) x 4 x 4\n",
    "        e8  = self.downconv8(self.downrelu(e7))                        # input e7 is (ngf * 8) x 2 x 2\n",
    "        d1_ = self.updrop1(self.upbn1(self.upconv1(self.uprelu(e8))))  # input e8 is (ngf * 8) x 1 x 1\n",
    "        d1  = torch.cat([d1_, e7], dim=1)\n",
    "        d2_ = self.updrop2(self.upbn2(self.upconv2(self.uprelu(d1))))  # input d1 is (ngf * 8 * 2) x 2 x 2\n",
    "        d2  = torch.cat([d2_, e6], dim=1)             \n",
    "        d3_ = self.updrop3(self.upbn3(self.upconv3(self.uprelu(d2))))  # input d2 is (ngf * 8 * 2) x 4 x 4\n",
    "        d3  = torch.cat([d3_, e5], dim=1)             \n",
    "        d4_ = self.upbn4(self.upconv4(self.uprelu(d3)))                # input d3 is (ngf * 8 * 2) x 8 x 8\n",
    "        d4  = torch.cat([d4_, e4], dim=1)             \n",
    "        d5_ = self.upbn5(self.upconv5(self.uprelu(d4)))                # input d4 is (ngf * 8 * 2) x 16 x 16\n",
    "        d5  = torch.cat([d5_, e3], dim=1)\n",
    "        d6_ = self.upbn6(self.upconv6(self.uprelu(d5)))                # input d5 is (ngf * 4 * 2) x 32 x 32\n",
    "        d6  = torch.cat([d6_, e2], dim=1)\n",
    "        d7_ = self.upbn7(self.upconv7(self.uprelu(d6)))                # input d6 is (ngf * 2 * 2) x 64 x 64\n",
    "        d7  = torch.cat([d7_, e1], dim=1)\n",
    "        d8  = self.upconv8(self.uprelu(d7))                            # input d7 is (ngf * 2) x 128 x 128\n",
    "        o1  = self.tanh(d8)                                            # input d8 is (output_nc) x 256 x 256\n",
    "        return o1\n",
    "\n",
    "net = Unet(3, 3)\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_128(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, use_bias=False, use_dropout=True):\n",
    "        \"\"\"\n",
    "        :param input_nc: number of input channels\n",
    "        :param output_nc: number of output channels\n",
    "        :param ngf: number of generator filters in the first convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.downrelu = nn.LeakyReLU(0.2, True)\n",
    "        self.uprelu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.drop_rate = 0.5 if use_dropout else 0.0\n",
    "        \n",
    "        self.downconv1 = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downconv2 = nn.Conv2d(ngf, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn2   = nn.BatchNorm2d(ngf*2)\n",
    "        self.downconv3 = nn.Conv2d(ngf*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn3   = nn.BatchNorm2d(ngf*4)\n",
    "        self.downconv4 = nn.Conv2d(ngf*4, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn4   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv5 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn5   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv6 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn6   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv7 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn7   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv8 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        \n",
    "        self.upconv1   = nn.ConvTranspose2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn1     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop1   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv2   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn2     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop2   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv3   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn3     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop3   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv4   = nn.ConvTranspose2d(ngf*8*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn4     = nn.BatchNorm2d(ngf*4)\n",
    "        self.upconv5   = nn.ConvTranspose2d(ngf*4*2, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn5     = nn.BatchNorm2d(ngf*2)\n",
    "        self.upconv6   = nn.ConvTranspose2d(ngf*2*2, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn6     = nn.BatchNorm2d(ngf*1)\n",
    "        self.upconv7   = nn.ConvTranspose2d(ngf*2, output_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        e1  = self.downconv1(x)                                        # input x is (input_nc) x 128 x 128\n",
    "        e2  = self.downbn2(self.downconv2(self.downrelu(e1)))          # input e1 is (ngf) x 64 x 64\n",
    "        e3  = self.downbn3(self.downconv3(self.downrelu(e2)))          # input e2 is (ngf * 2) x 32 x 32\n",
    "        e4  = self.downbn4(self.downconv4(self.downrelu(e3)))          # input e3 is (ngf * 4) x 16 x16\n",
    "        e5  = self.downbn5(self.downconv5(self.downrelu(e4)))          # input e4 is (ngf * 8) x 8 x 8\n",
    "        e6  = self.downbn6(self.downconv6(self.downrelu(e5)))          # input e5 is (ngf * 8) x 4 x 4\n",
    "        e7  = self.downconv7(self.downrelu(e6))                        # input e6 is (ngf * 8) x 2 x 2\n",
    "\n",
    "        d1_ = self.updrop1(self.upbn1(self.upconv1(self.uprelu(e7))))  # input e7 is (ngf * 8) x 1 x 1\n",
    "        d1  = torch.cat([d1_, e6], dim=1)\n",
    "        d2_ = self.updrop2(self.upbn2(self.upconv2(self.uprelu(d1))))  # input d1 is (ngf * 8 * 2) x 2 x 2\n",
    "        d2  = torch.cat([d2_, e5], dim=1)             \n",
    "        d3_ = self.updrop3(self.upbn3(self.upconv3(self.uprelu(d2))))  # input d2 is (ngf * 8 * 2) x 4 x 4\n",
    "        d3  = torch.cat([d3_, e4], dim=1)             \n",
    "        d4_ = self.upbn4(self.upconv4(self.uprelu(d3)))                # input d3 is (ngf * 8 * 2) x 8 x 8\n",
    "        d4  = torch.cat([d4_, e3], dim=1)      \n",
    "        d5_ = self.upbn5(self.upconv5(self.uprelu(d4)))                # input d4 is (ngf * 8 * 2) x 16 x 16\n",
    "        d5  = torch.cat([d5_, e2], dim=1)\n",
    "        d6_ = self.upbn6(self.upconv6(self.uprelu(d5)))                # input d5 is (ngf * 4 * 2) x 32 x 32\n",
    "        d6  = torch.cat([d6_, e1], dim=1)\n",
    "        d7 =  self.upconv7(self.uprelu(d6))                            # input d6 is (ngf * 2 * 2) x 64 x 64\n",
    "                                                                       # input d7 is (ngf * 2) x 128 x 128\n",
    "        o1  = self.tanh(d7)                                            \n",
    "        return o1\n",
    "\n",
    "net = Unet_128(3, 3)\n",
    "x = torch.randn(1, 3, 128, 128)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64):\n",
    "        \"\"\"\n",
    "        :param input_nc: number of input channels\n",
    "        :param ndf: number of discriminator filters in the first convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
    "        \n",
    "        self.conv1    = nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1)   \n",
    "        self.conv2    = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)       \n",
    "        self.conv2_bn = nn.BatchNorm2d(ndf*2)\n",
    "        self.conv3    = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)   \n",
    "        self.conv3_bn = nn.BatchNorm2d(ndf*4)\n",
    "        self.conv4    = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=1, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(ndf*8)\n",
    "        self.conv5    = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = self.leaky_relu(self.conv4_bn(self.conv4(x)))\n",
    "        x = self.conv5(x)  # No sigmoid since BCEWithLogitsLoss is used\n",
    "        return x\n",
    "\n",
    "net = Discriminator(3)\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64):\n",
    "        \"\"\"\n",
    "        :param input_nc: number of input channels\n",
    "        :param ndf: number of discriminator filters in the first convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
    "        \n",
    "        self.conv1    = nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1)   \n",
    "        self.conv2    = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)       \n",
    "        self.conv2_n  = nn.InstanceNorm2d(ndf*2)\n",
    "        self.conv3    = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)   \n",
    "        self.conv3_n  = nn.InstanceNorm2d(ndf*4)\n",
    "        self.conv4    = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=1, padding=1)\n",
    "        self.conv4_n  = nn.InstanceNorm2d(ndf*8)\n",
    "        self.conv5    = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2_n(self.conv2(x)))\n",
    "        x = self.leaky_relu(self.conv3_n(self.conv3(x)))\n",
    "        x = self.leaky_relu(self.conv4_n(self.conv4(x)))\n",
    "        x = self.conv5(x)  # No sigmoid since Critic\n",
    "        return x\n",
    "net = Critic(3)\n",
    "x = torch.randn(1, 3, 128, 128)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "    \n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "            \n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel(nn.Module):\n",
    "    def __init__(self, generator_lr=2e-4, discriminator_lr=2e-4, color_space=\"Lab\", lambda_coef=100.0, betas=(0.5, 0.999)):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.color_space = color_space\n",
    "        if self.color_space == \"Lab\":\n",
    "            out_ch = 2\n",
    "        else:\n",
    "            out_ch = 3\n",
    "\n",
    "        self.generator = init_model(Unet(input_nc=1, output_nc=out_ch), self.device)\n",
    "        self.discriminator = init_model(Discriminator(input_nc=3), self.device)\n",
    "        \n",
    "        self.GANLoss = nn.BCEWithLogitsLoss()\n",
    "        self.L1Loss = nn.L1Loss()\n",
    "        self.lambda_coef = lambda_coef\n",
    "        \n",
    "        self.generator_optimizer = optim.Adam(self.generator.parameters(), lr=generator_lr, betas=betas)\n",
    "        self.discriminator_optimizer = optim.Adam(self.discriminator.parameters(), lr=discriminator_lr, betas=betas)\n",
    "        \n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def save(self, epoch, log, path=\"./checkpoint.pt\", download=True):\n",
    "        \"\"\"Saves state_dict for generator, discriminator and optimizers to path \n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'epoch' : epoch,\n",
    "            'log' : log,\n",
    "            'generator_state_dict' : self.generator.state_dict(),\n",
    "            'discriminator_state_dict' : self.discriminator.state_dict(),\n",
    "            'generator_optimizer_state_dict' : self.generator_optimizer.state_dict(),\n",
    "            'discriminator_optimizer_state_dict' : self.discriminator_optimizer.state_dict()\n",
    "        }, path)\n",
    "        \n",
    "        if download:\n",
    "            # Only works on Colab\n",
    "            files.download(path)   \n",
    "            \n",
    "    def load(self, path=\"./checkpoint.pt\"):\n",
    "        \"\"\"Loads state_dict for generator, discriminator and optimizers from path \n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.generator_optimizer.load_state_dict(checkpoint['generator_optimizer_state_dict'])\n",
    "        self.discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['log']\n",
    "\n",
    "            \n",
    "model = GANModel()   \n",
    "model.save(epoch=0, download=False)\n",
    "ep = model.load()\n",
    "print(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W-GAN with Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANModel(nn.Module):\n",
    "    def __init__(self, generator_lr=1e-4, discriminator_lr=1e-4, color_space=\"Lab\", n_critic_iterations=5, lambda_gp=10.0, lambda_L1=0.0, betas=(0.0, 0.9)):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.color_space = color_space\n",
    "        if self.color_space == \"Lab\":\n",
    "            out_ch = 2\n",
    "        else:\n",
    "            out_ch = 3\n",
    "\n",
    "        self.generator = init_model(Unet_128(input_nc=1, output_nc=out_ch), self.device)\n",
    "        self.critic = init_model(Critic(input_nc=3), self.device)\n",
    "\n",
    "        self.n_critic_iterations = n_critic_iterations\n",
    "        self.L1Loss = nn.L1Loss()\n",
    "        self.lambda_L1 = lambda_L1\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.PerceptualLoss = LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(self.device)\n",
    "        \n",
    "        self.generator_optimizer = optim.Adam(self.generator.parameters(), lr=generator_lr, betas=betas)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=discriminator_lr, betas=betas)\n",
    "        \n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def save(self, epoch, log, path=\"./checkpoint.pt\", download=True):\n",
    "        \"\"\"Saves state_dict for generator, discriminator and optimizers to path \n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'epoch' : epoch,\n",
    "            'log' : log,\n",
    "            'generator_state_dict' : self.generator.state_dict(),\n",
    "            'discriminator_state_dict' : self.critic.state_dict(),\n",
    "            'generator_optimizer_state_dict' : self.generator_optimizer.state_dict(),\n",
    "            'discriminator_optimizer_state_dict' : self.critic_optimizer.state_dict()\n",
    "        }, path)\n",
    "        \n",
    "        if download:\n",
    "            files.download(path)\n",
    "            \n",
    "    def load(self, path=\"./checkpoint.pt\"):\n",
    "        \"\"\"Loads state_dict for generator, discriminator and optimizers from path \n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.critic.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.generator_optimizer.load_state_dict(checkpoint['generator_optimizer_state_dict'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['log']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, batch, color_space=\"Lab\"):\n",
    "    model.generator.eval()\n",
    "    with torch.no_grad():\n",
    "        x, label = batch\n",
    "        x = x.to(model.device)\n",
    "        output = model.generator(x)\n",
    "    \n",
    "        x = x.cpu()\n",
    "        label = label.cpu()\n",
    "        output = output.detach().cpu()\n",
    "\n",
    "        x = (x + 1.0) / 2.0\n",
    "        output = (output + 1.0) / 2.0\n",
    "        label = (label + 1.0) / 2.0\n",
    "    model.generator.train()\n",
    "\n",
    "    if color_space == \"Lab\":\n",
    "        generated_images = torch.cat([x, output], dim=1)\n",
    "        true_images = torch.cat([x, label], dim=1)\n",
    "    else:\n",
    "        generated_images = output\n",
    "        true_images = label\n",
    "\n",
    "    num_images = generated_images.shape[0]\n",
    "\n",
    "    fig, ax = plt.subplots(3,num_images, figsize=(20,15))\n",
    "\n",
    "    i = 0\n",
    "    for inp, img, true_img in zip(x, generated_images, true_images):\n",
    "        if color_space == \"Lab\":\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            img = np.array(img)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_Lab2RGB)\n",
    "            true_img = transforms.ToPILImage()(true_img)\n",
    "            true_img = np.array(true_img)\n",
    "            true_img = cv2.cvtColor(true_img, cv2.COLOR_Lab2RGB)\n",
    "        ax[0,i].imshow(transforms.ToPILImage()(inp), cmap='gray')\n",
    "        ax[1,i].imshow(transforms.ToPILImage()(img))\n",
    "        ax[2,i].imshow(transforms.ToPILImage()(true_img))\n",
    "        ax[0,i].axis(\"off\")\n",
    "        ax[1,i].axis(\"off\")\n",
    "        ax[2,i].axis(\"off\")\n",
    "        i+=1\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_penalty(model, real_images, generated_images):\n",
    "    epsilon = torch.rand(real_images.shape[0], 1, device=model.device)\n",
    "    epsilon = epsilon.expand(real_images.shape[0], real_images.nelement() // real_images.shape[0]).contiguous().view(*real_images.shape)\n",
    "    interpolated = epsilon * real_images + (1 - epsilon) * generated_images\n",
    "    interpolated.requires_grad_(True)\n",
    "    interpolated_outputs = model.critic(interpolated)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_outputs,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones(interpolated_outputs.size()).to(model.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )\n",
    "    gradients = gradients[0].view(real_images.size(0), -1) \n",
    "    gradients_norm = (gradients + 1e-16).norm(2, dim=1)\n",
    "    gradient_penalty = ((gradients_norm - 1.0)**2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANModel(color_space=COLOR_SPACE)\n",
    "for batch in val_dl:\n",
    "    visualize(model, batch, COLOR_SPACE)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, val_dl, test_dl, epochs=100, show_every=100, checkpoint=None):\n",
    "    test_dl_iterator = iter(test_dl)\n",
    "    if checkpoint:\n",
    "        curr_ep, log = model.load(checkpoint)\n",
    "    else:\n",
    "        log = {'tr_generator_loss' : [], 'tr_discriminator_loss' : [], 'val_generator_loss' : [], 'val_discriminator_loss' : []}\n",
    "        curr_ep = 0\n",
    "    for ep in range(epochs):\n",
    "        ep_num = (curr_ep+ep+1)\n",
    "        b_n = 0\n",
    "        ep_gen_tr_loss = []\n",
    "        ep_dis_tr_loss = []\n",
    "        ep_gen_val_loss = []\n",
    "        ep_dis_val_loss = []\n",
    "\n",
    "        model.generator.train()\n",
    "        model.discriminator.train()\n",
    "        for batch in tqdm(train_dl):\n",
    "            x, label = batch\n",
    "            \n",
    "            x = x.to(model.device)\n",
    "            label = label.to(model.device)\n",
    "            \n",
    "            generator_output = model.generator(x)\n",
    "            \n",
    "            model.discriminator.train()\n",
    "            model.set_requires_grad(model.discriminator, True)\n",
    "            model.discriminator_optimizer.zero_grad()\n",
    "            \n",
    "            if model.color_space == \"Lab\":\n",
    "                generated_images = torch.cat([x, generator_output], dim=1)\n",
    "                true_images = torch.cat([x, label], dim=1)\n",
    "            else:\n",
    "                generated_images = generator_output\n",
    "                true_images = label\n",
    "            \n",
    "            prediction_generated_images = model.discriminator(generated_images.detach())\n",
    "            zeros = torch.tensor(0.0).expand_as(prediction_generated_images).to(model.device)\n",
    "            loss_generated_images = model.GANLoss(prediction_generated_images, zeros)\n",
    "            \n",
    "            prediction_true_images = model.discriminator(true_images)\n",
    "            ones = torch.tensor(1.0).expand_as(prediction_true_images).to(model.device)\n",
    "            loss_true_images = model.GANLoss(prediction_true_images, ones)\n",
    "            \n",
    "            discriminator_loss = 0.5 * (loss_generated_images + loss_true_images)\n",
    "            discriminator_loss.backward()\n",
    "            \n",
    "            model.discriminator_optimizer.step()\n",
    "            \n",
    "            model.generator.train()\n",
    "            model.set_requires_grad(model.discriminator, False)\n",
    "            model.generator_optimizer.zero_grad()\n",
    "            \n",
    "            prediction_generated_images = model.discriminator(generated_images)\n",
    "            ones = torch.tensor(1.0).expand_as(prediction_generated_images).to(model.device)\n",
    "            loss_generated_images = model.GANLoss(prediction_generated_images, ones)\n",
    "            loss_L1 = model.L1Loss(generator_output, label)\n",
    "            generator_loss = loss_generated_images + loss_L1 * model.lambda_coef\n",
    "            generator_loss.backward()\n",
    "            \n",
    "            model.generator_optimizer.step()\n",
    "\n",
    "            ep_dis_tr_loss.append(discriminator_loss.detach().cpu().numpy())\n",
    "            ep_gen_tr_loss.append(generator_loss.detach().cpu().numpy())\n",
    "            \n",
    "            b_n += 1\n",
    "            if b_n % show_every == 0:\n",
    "                b = next(iter(test_dl_iterator))\n",
    "                visualize(model, b, COLOR_SPACE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.generator.eval()\n",
    "            model.discriminator.eval()\n",
    "            for batch in tqdm(val_dl):\n",
    "                x, label = batch\n",
    "            \n",
    "                x = x.to(model.device)\n",
    "                label = label.to(model.device)\n",
    "            \n",
    "                generator_output = model.generator(x)\n",
    "                if model.color_space == \"Lab\":\n",
    "                    generated_images = torch.cat([x, generator_output], dim=1)\n",
    "                    true_images = torch.cat([x, label], dim=1)\n",
    "                else:\n",
    "                    generated_images = generator_output\n",
    "                    true_images = label\n",
    "\n",
    "                prediction_generated_images = model.discriminator(generated_images.detach())\n",
    "                zeros = torch.tensor(0.0).expand_as(prediction_generated_images).to(model.device)\n",
    "                loss_generated_images = model.GANLoss(prediction_generated_images, zeros)\n",
    "            \n",
    "                prediction_true_images = model.discriminator(true_images)\n",
    "                ones = torch.tensor(1.0).expand_as(prediction_true_images).to(model.device)\n",
    "                loss_true_images = model.GANLoss(prediction_true_images, ones)\n",
    "            \n",
    "                discriminator_loss = 0.5 * (loss_generated_images + loss_true_images)\n",
    "            \n",
    "                prediction_generated_images = model.discriminator(generated_images)\n",
    "                ones = torch.tensor(1.0).expand_as(prediction_generated_images).to(model.device)\n",
    "                loss_generated_images = model.GANLoss(prediction_generated_images, ones)\n",
    "                loss_L1 = model.L1Loss(generator_output, label)\n",
    "                generator_loss = loss_generated_images + loss_L1 * model.lambda_coef\n",
    "            \n",
    "                ep_dis_val_loss.append(discriminator_loss.detach().cpu().numpy())\n",
    "                ep_gen_val_loss.append(generator_loss.detach().cpu().numpy())\n",
    "        \n",
    "        tr_D_loss = np.mean(ep_dis_tr_loss)\n",
    "        tr_G_loss = np.mean(ep_gen_tr_loss)\n",
    "        val_D_loss = np.mean(ep_dis_val_loss)\n",
    "        val_G_loss = np.mean(ep_gen_val_loss)\n",
    "    \n",
    "        log['tr_discriminator_loss'].append(tr_D_loss)\n",
    "        log['tr_generator_loss'].append(tr_G_loss)\n",
    "        log['val_discriminator_loss'].append(val_D_loss)\n",
    "        log['val_generator_loss'].append(val_G_loss)\n",
    "\n",
    "        print(f\"EPOCH {ep_num}\", end=\"\")\n",
    "        print(\"-\"*100)\n",
    "        print(f\"discriminator tr_loss:{tr_D_loss} generator tr_loss:{tr_G_loss}\")\n",
    "        print(f\"discriminator val_loss:{val_D_loss} generator val_loss:{val_G_loss}\")\n",
    "        model.save(epoch=ep_num, log=log, path=f\"./checkpoint.pt\", download=False)\n",
    "        print(f\"SAVED CHECKPOINT EPOCH {ep_num}\")\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wgan(model, train_dl, val_dl, test_dl, epochs=100, show_every=100, checkpoint=None, use_perceptual_loss=True):\n",
    "    test_dl_iterator = iter(test_dl)\n",
    "    if checkpoint:\n",
    "        curr_ep, log = model.load(checkpoint)\n",
    "    else:\n",
    "        log = {'tr_generator_loss' : [], 'tr_discriminator_loss' : [], 'val_generator_loss' : [], 'val_discriminator_loss' : []}\n",
    "        curr_ep = 0\n",
    "    for ep in range(epochs):\n",
    "        ep_num = (curr_ep+ep+1)\n",
    "        b_n = 0\n",
    "        ep_gen_tr_loss = []\n",
    "        ep_dis_tr_loss = []\n",
    "        ep_gen_val_loss = []\n",
    "        ep_dis_val_loss = []\n",
    "\n",
    "        model.generator.train()\n",
    "        model.critic.train()\n",
    "        for batch in tqdm(train_dl):\n",
    "            x, label = batch\n",
    "            \n",
    "            x = x.to(model.device)\n",
    "            label = label.to(model.device)\n",
    "            \n",
    "            model.critic.train()\n",
    "            model.set_requires_grad(model.critic, True)\n",
    "            for _ in range(model.n_critic_iterations):\n",
    "                generator_output = model.generator(x)\n",
    "                model.critic_optimizer.zero_grad()\n",
    "            \n",
    "                if model.color_space == \"Lab\":\n",
    "                    generated_images = torch.cat([x, generator_output], dim=1)\n",
    "                    true_images = torch.cat([x, label], dim=1)\n",
    "                else:\n",
    "                    generated_images = generator_output\n",
    "                    true_images = label\n",
    "            \n",
    "                prediction_generated_images = model.critic(generated_images)\n",
    "                loss_generated_images = prediction_generated_images.mean()\n",
    "                prediction_true_images = model.critic(true_images)\n",
    "                loss_true_images = -prediction_true_images.mean()\n",
    "                gradient_penalty_loss = get_gradient_penalty(model, true_images, generated_images)\n",
    "                discriminator_loss = loss_generated_images + loss_true_images + model.lambda_gp * gradient_penalty_loss\n",
    "                discriminator_loss.backward(retain_graph=True)\n",
    "                model.critic_optimizer.step()\n",
    "            \n",
    "            model.generator.train()\n",
    "            model.set_requires_grad(model.critic, False)\n",
    "            model.generator_optimizer.zero_grad()\n",
    "            \n",
    "            prediction_generated_images = model.critic(generated_images)\n",
    "            loss_generated_images = -prediction_generated_images.mean()\n",
    "            loss_L1 = model.L1Loss(generator_output, label)\n",
    "            generator_loss = loss_generated_images + loss_L1 * model.lambda_L1\n",
    "            if use_perceptual_loss:\n",
    "                perceptual_loss = model.PerceptualLoss(generated_images, true_images)\n",
    "                generator_loss += perceptual_loss\n",
    "            generator_loss.backward()\n",
    "            \n",
    "            model.generator_optimizer.step()\n",
    "\n",
    "            ep_dis_tr_loss.append(discriminator_loss.detach().cpu().numpy())\n",
    "            ep_gen_tr_loss.append(generator_loss.detach().cpu().numpy())\n",
    "            \n",
    "            b_n += 1\n",
    "            if b_n % show_every == 0:\n",
    "                b = next(test_dl_iterator)\n",
    "                visualize(model, b, COLOR_SPACE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.generator.eval()\n",
    "            model.critic.eval()\n",
    "            for batch in tqdm(val_dl):\n",
    "                x, label = batch\n",
    "            \n",
    "                x = x.to(model.device)\n",
    "                label = label.to(model.device)\n",
    "            \n",
    "                generator_output = model.generator(x)\n",
    "                if model.color_space == \"Lab\":\n",
    "                    generated_images = torch.cat([x, generator_output], dim=1)\n",
    "                    true_images = torch.cat([x, label], dim=1)\n",
    "                else:\n",
    "                    generated_images = generator_output\n",
    "                    true_images = label\n",
    "\n",
    "                prediction_generated_images = model.critic(generated_images)\n",
    "                loss_generated_images = prediction_generated_images.mean()\n",
    "            \n",
    "                prediction_true_images = model.critic(true_images)\n",
    "                loss_true_images = -prediction_true_images.mean()\n",
    "            \n",
    "                discriminator_loss = 0.5 * (loss_generated_images + loss_true_images)\n",
    "            \n",
    "                prediction_generated_images = model.critic(generated_images)\n",
    "                loss_generated_images = -prediction_generated_images.mean()\n",
    "                loss_L1 = model.L1Loss(generator_output, label)\n",
    "                generator_loss = loss_generated_images + loss_L1 * model.lambda_L1\n",
    "\n",
    "                if use_perceptual_loss:\n",
    "                    perceptual_loss = model.PerceptualLoss(generated_images, true_images)\n",
    "                    generator_loss += perceptual_loss\n",
    "\n",
    "                ep_dis_val_loss.append(discriminator_loss.detach().cpu().numpy())\n",
    "                ep_gen_val_loss.append(generator_loss.detach().cpu().numpy())\n",
    "        \n",
    "        tr_D_loss = np.mean(ep_dis_tr_loss)\n",
    "        tr_G_loss = np.mean(ep_gen_tr_loss)\n",
    "        val_D_loss = np.mean(ep_dis_val_loss)\n",
    "        val_G_loss = np.mean(ep_gen_val_loss)\n",
    "    \n",
    "        log['tr_discriminator_loss'].append(tr_D_loss)\n",
    "        log['tr_generator_loss'].append(tr_G_loss)\n",
    "        log['val_discriminator_loss'].append(val_D_loss)\n",
    "        log['val_generator_loss'].append(val_G_loss)\n",
    "\n",
    "        print(f\"EPOCH {ep_num}\", end=\"\")\n",
    "        print(\"-\"*100)\n",
    "        print(f\"discriminator tr_loss:{tr_D_loss} generator tr_loss:{tr_G_loss}\")\n",
    "        print(f\"discriminator val_loss:{val_D_loss} generator val_loss:{val_G_loss}\")\n",
    "        model.save(epoch=ep_num, log=log, path=f\"./checkpoint.pt\", download=False)\n",
    "        print(f\"SAVED CHECKPOINT EPOCH {ep_num}\")\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANModel(color_space=COLOR_SPACE)\n",
    "checkpoint = None\n",
    "train(model, train_dl, val_dl, test_dl, epochs=100, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training the model\n",
    "model = GANModel(color_space=COLOR_SPACE)\n",
    "checkpoint = \"./checkpoint.pt\"\n",
    "train(model, train_dl, val_dl, test_dl, epochs=100, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pre-trained Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def pretrain_generator(generator, train_dl, optimizer, loss_fn=nn.L1Loss(), epochs=20):\n",
    "    log = {'training_loss' : []}\n",
    "    for ep in range(epochs):\n",
    "        ep_loss = []\n",
    "        generator.train()\n",
    "        for batch in tqdm(train_dl):\n",
    "            x, label = batch\n",
    "            \n",
    "            x = x.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            \n",
    "            generator_output = generator(x)\n",
    "            \n",
    "            loss = loss_fn(generator_output, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            b_loss = loss.detach().cpu().numpy()\n",
    "            ep_loss.append(b_loss)\n",
    "        \n",
    "        episode_loss = np.mean(ep_loss)\n",
    "        log['training_loss'].append(episode_loss)\n",
    "        print(f\"EPOCH {ep} training_loss: {episode_loss}\")\n",
    "        torch.save(generator.state_dict(), \"pretrained_generator.pt\")\n",
    "        print(f\"SAVED MODEL\")\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18()\n",
    "body = create_body(model=net, pretrained=True, n_in=1, cut=-2)\n",
    "generator = DynamicUnet(body, 2, (256, 256)).to(DEVICE)\n",
    "optimizer = optim.Adam(generator.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_generator(generator, train_dl, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANModel(color_space=COLOR_SPACE)\n",
    "model.generator = generator\n",
    "model.generator_optimizer = optim.Adam(model.generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "checkpoint = \"./checkpoint.pt\"\n",
    "train(model, train_dl, val_dl, test_dl, epochs=10, checkpoint=checkpoint, show_every=313, alpha=0.1, use_perceptual_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint= \"./checkpoint.pt\"\n",
    "ep, log = model.load(checkpoint)\n",
    "print(ep)\n",
    "it = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(it)\n",
    "visualize(model, b, COLOR_SPACE)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
